# -*- coding: utf-8 -*-
"""Copy of Copy of FFNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15vW3IWagOX1XT_JotK8dEP0-2IM3WY6V
"""

import pandas as pd
import numpy as np
import os
from gensim import corpora,models,similarities
from sklearn.model_selection import train_test_split

#os.getcwd()

dataF = pd.read_excel("original+bt_data.xlsx",names=['Text_id', 'text_data','profane_class'])

dataF.head(10)

print("Columns in the original dataset:\n")
print(dataF.columns)

import matplotlib.pyplot as plt 

print("Number of rows per star rating:")
print(dataF['profane_class'].value_counts())

# Function to map stars to sentiment
def map_sentiment(profane_class):
    if profane_class == 1:
        return 1
    elif profane_class == 0:
        return 0
    else:
        return -1
# Mapping stars to sentiment into three categories
dataF['sentiment'] = [ map_sentiment(x) for x in dataF['profane_class']]
# Plotting the sentiment distribution
plt.figure()
pd.value_counts(dataF['sentiment']).plot.bar(title="Sentiment distribution in dF")
plt.xlabel("Sentiment")
plt.ylabel("No. of rows in dF")
plt.show()

# Removing the stop words
from gensim.parsing.preprocessing import remove_stopwords
print(remove_stopwords("They had a good service!!"))

from gensim.utils import simple_preprocess
# Tokenize the text column to get the new column 'tokenized_text'
dataF['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in dataF['text_data']] 
print(dataF['tokenized_text'].head(10))

from gensim.parsing.porter import PorterStemmer
porter_stemmer = PorterStemmer()
# Get the stemmed_tokens
dataF['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in dataF['tokenized_text'] ]
dataF['stemmed_tokens'].head(10)

from sklearn.model_selection import train_test_split
# Train Test Split Function
def split_train_test(dataF, test_size=0.3, shuffle_state=True):
    X_train, X_test, Y_train, Y_test = train_test_split(dataF[['Text_id','text_data','profane_class','stemmed_tokens']], 
                                                        dataF['sentiment'], 
                                                        shuffle=shuffle_state,
                                                        test_size=test_size, 
                                                        random_state=15)
    print("Value counts for Train sentiments")
    print(Y_train.value_counts())
    print("Value counts for Test sentiments")
    print(Y_test.value_counts())
    print(type(X_train))
    print(type(Y_train))
    X_train = X_train.reset_index()
    X_test = X_test.reset_index()
    Y_train = Y_train.to_frame()
    Y_train = Y_train.reset_index()
    Y_test = Y_test.to_frame()
    Y_test = Y_test.reset_index()
    print(X_train.head())
    return X_train, X_test, Y_train, Y_test

# Call the train_test_split
X_train, X_test, Y_train, Y_test = split_train_test(dataF)

import torch.nn as nn
import torch.nn.functional as FF
import torch.optim as optim

import torch
# Use cuda if present
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device available for running: ")
print(device)

class FeedforwardNeuralNetModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(FeedforwardNeuralNetModel, self).__init__()
        
        # Linear function 1: vocab_size --> 500
        self.fc1 = nn.Linear(input_dim, hidden_dim) 
        # Non-linearity 1
        self.relu1 = nn.ReLU()

        # Linear function 2: 500 --> 500
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        # Non-linearity 2
        self.relu2 = nn.ReLU()

        # Linear function 3 (readout): 500 --> 3
        self.fc3 = nn.Linear(hidden_dim, output_dim)  

    def forward(self, x):
        # Linear function 1
        out = self.fc1(x)
        # Non-linearity 1
        out = self.relu1(out)

        # Linear function 2
        out = self.fc2(out)
        # Non-linearity 2
        out = self.relu2(out)

        # Linear function 3 (readout)
        out = self.fc3(out)

        return FF.softmax(out, dim=1)

from gensim import corpora
# Function to return the dictionary either with padding word or without padding
def m_dict(dataF, padding=True):
    if padding:
        print("Dictionary with padded token added")
        print(dataF)
        r_dict = corpora.Dictionary([['pad']])
        r_dict.add_documents(dataF['stemmed_tokens'])
    else:
        print("Dictionary without padding")
        print(dataF)
        r_dict = corpora.Dictionary(dataF['stemmed_tokens'])
    return r_dict
# Make the dictionary without padding for the basic models
r_dict = m_dict(dataF, padding=False)

VOCAB_SIZE = len(r_dict)
NUM_LABELS = 3

# Function to make bow vector to be used as input to network
def make_bow_vector(r_dict, sentence):
    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)
    for word in sentence:
        vec[r_dict.token2id[word]] += 1
    return vec.view(1, -1).float()

# Function to get the output tensor
def make_target(label):
    if label == -1:
        return torch.tensor([0], dtype=torch.long, device=device)
    elif label == 0:
        return torch.tensor([1], dtype=torch.long, device=device)
    else:
        return torch.tensor([2], dtype=torch.long, device=device)

VOCAB_SIZE = len(r_dict)

input_dim = VOCAB_SIZE
hidden_dim = 500
output_dim = 3
num_epochs = 100

ff_nn_bow_model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)
ff_nn_bow_model.to(device)

loss_function = nn.CrossEntropyLoss()
optimizer = optim.SGD(ff_nn_bow_model.parameters(), lr=0.001)

import csv

# Open the file for writing loss
ffnn_loss_file_name = 'final.csv'
f = open(ffnn_loss_file_name,'w')
f.write('iter, loss')
f.write('\n')
losses = []
iter = 0
# Start training
for epoch in range(num_epochs):
    #print("1")
    if (epoch+1) % 1 == 0:
  
      #print("2")
      print("Epoch completed: " + str(epoch+1))
    train_loss = 0
    for index, row in X_train.iterrows():
      if index % 1000 == 0:
        # Clearing the accumulated gradients
        optimizer.zero_grad()
        #print(index)
        #print(row)
        # Make the bag of words vector for stemmed tokens 
        bow_vec = make_bow_vector(r_dict, row['stemmed_tokens'])
       
        # Forward pass to get output
        probs = ff_nn_bow_model(bow_vec)

        # Get the target label
        target = make_target(Y_train['sentiment'][index])

        # Calculate Loss: softmax --> cross entropy loss
        loss = loss_function(probs, target)
        # Accumulating the loss over time
        train_loss += loss.item()
        #print("3")
        optimizer.zero_grad()
        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()
       

    with open('final.csv', 'w') as csvfile:
      cwriter = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)
      cwriter.writerow(str((epoch+1)) + "," + str(train_loss / len(X_train)))

    

    train_loss = 0

    f.close()

from sklearn.metrics import classification_report
bow_ff_nn_predictions = []
original_lables_ff_bow = []
with torch.no_grad():
    for index, row in X_test.iterrows():
        bow_vec = make_bow_vector(r_dict, row['stemmed_tokens'])
        probs = ff_nn_bow_model(bow_vec)
        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])
        original_lables_ff_bow.append(make_target(Y_test['sentiment'][index]).cpu().numpy()[0])
print(classification_report(original_lables_ff_bow,bow_ff_nn_predictions))
ffnn_loss_df = pd.read_csv(ffnn_loss_file_name)


print(len(ffnn_loss_df))
print(ffnn_loss_df.columns)
#ffnn_plt_500_padding_100_epochs = ffnn_loss_df[' loss'].plot()
#fig = ffnn_plt_500_padding_100_epochs.get_figure()
#fig