{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required for the BT model\n",
    "# !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "import re\n",
    "import timeit\n",
    "#import nltk\n",
    "#from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv files\n",
    "tweet_input = pd.read_csv('C:/Users/saath/Documents/DAEN 690/datasets/train1.csv')\n",
    "comment_input = pd.read_csv('C:/Users/saath/Documents/DAEN 690/datasets/train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "tweet_input_final = tweet_input.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1)\n",
    "comment_input_final = comment_input.drop(['id', 'toxic', 'severe_toxic', 'threat','insult', 'identity_hate'],axis=1)\n",
    "\n",
    "# Reordering the columns in the tweet_input\n",
    "tweet_input_final = tweet_input_final.reindex(['tweet','class'], axis=1)\n",
    "\n",
    "#Renaming the columns into a common name\n",
    "comment_input_final.rename(columns={'comment_text': 'text_data', 'obscene': 'profane_class'}, inplace=True)\n",
    "tweet_input_final.rename(columns={'tweet': 'text_data', 'class': 'profane_class'}, inplace=True)\n",
    "\n",
    "#Considering only the profane and non profane classes\n",
    "tweet_input_final = tweet_input_final[tweet_input_final.profane_class != 0]\n",
    "tweet_input_final['profane_class'] = tweet_input_final['profane_class'].replace([2],0)\n",
    "# tweet_input_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data by removing punctuations, numeric data and http links\n",
    "# comment_input_final['text_data'] = comment_input_final.text_data.str.replace(r'[^a-zA-Z\\s]+@\\w+:|@\\w+|&\\w+|[^a-zA-Z\\s]+|RT|http.+?', '')\n",
    "# comment_input_final['text_data'] = comment_input_final.text_data.str.replace(r'\\n',' ')\n",
    "# comment_input_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input_final['text_data'] = tweet_input_final.text_data.str.replace(r'[^a-zA-Z\\s]+@\\w+:|@\\w+|&\\w+|[^a-zA-Z\\s]+|RT|http.+?', '')\n",
    "tweet_input_final['text_data'] = tweet_input_final.text_data.str.replace(r'\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(from_model_name='Helsinki-NLP/opus-mt-en-es', \n",
    "                                              to_model_name='Helsinki-NLP/opus-mt-es-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "actual_data = tweet_input_final[tweet_input_final.profane_class == 1]['text_data'].to_list()\n",
    "transformed_data = back_translation_aug.augment(actual_data)\n",
    "stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timeit.default_timer()\n",
    "# actual_data = comment_input_final[comment_input_final.profane_class == 1]['text_data'].to_list()\n",
    "# transformed_data_comment = back_translation_aug.augment(actual_data)\n",
    "# stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_tweet_data = pd.DataFrame()\n",
    "# transformed_comment_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = pd.Series(transformed_data)\n",
    "transformed_tweet_data['text_data'] = se.values\n",
    "transformed_tweet_data.insert(1,'profane_class',1)\n",
    "print(transformed_tweet_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se = pd.Series(transformed_data_comment)\n",
    "# transformed_comment_data['text_data'] = se.values\n",
    "# transformed_comment_data.insert(1,'profane_class',1)\n",
    "# print(transformed_comment_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_tweet_data.head(10)\n",
    "writer = pd.ExcelWriter('C:/Users/saath/Documents/DAEN 690/datasets/tweet_translated.xlsx')\n",
    "# write dataframe to excel\n",
    "transformed_tweet_data.to_excel(writer)\n",
    "# save the excel\n",
    "writer.save()\n",
    "print('DataFrame is written successfully to Excel File.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transformed_tweet_data.head(10)\n",
    "# writer = pd.ExcelWriter('C:/Users/saath/Documents/DAEN 690/datasets/comment_translated.xlsx')\n",
    "# # write dataframe to excel\n",
    "# transformed_comment_data.to_excel(writer)\n",
    "# # save the excel\n",
    "# writer.save()\n",
    "# print('DataFrame is written successfully to Excel File.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
