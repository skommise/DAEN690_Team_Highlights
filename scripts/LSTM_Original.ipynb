{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = pd.read_csv(r\"C:\\Users\\hvvel\\Downloads/combined_original_clean_data.csv\",engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55802, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           ID                                          text_data  profane_class\n",
       "15759  15759    These hoes really would sit in your face and...              1\n",
       "49080  49080   bullshiti guess the poor have all the moneyta...              1\n",
       "35265  35265  The details may help the aid of banning this u...              0\n",
       "33727  33727  There is disagreement regarding whether Norman...              0\n",
       "39085  39085                Thats hoser eh to you tcoQuLrtOWmLB              0\n",
       "...      ...                                                ...            ...\n",
       "49733  49733                   Hitler is also dead Coincidence               0\n",
       "14390  14390  Great  If you had not removed it I was going t...              0\n",
       "51224  51224   unblock  reason  Derogatory comments about ot...              0\n",
       "36696  36696  Shut up I can do what the fuck I want   July  ...              1\n",
       "10069  10069  The Rock isnt a blood relation of any of the A...              0\n",
       "\n",
       "[55802 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = cap_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hvvel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df[\"text_data\"] = cap_df.text_data.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(cap_df.text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87527"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fuck', 9878),\n",
       " ('bitch', 9438),\n",
       " ('article', 9051),\n",
       " ('page', 8372),\n",
       " ('like', 8258)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into training and test set\n",
    "random.seed(100)\n",
    "train_size = int(cap_df.shape[0] * 0.7)\n",
    "\n",
    "train_df = cap_df[:train_size]\n",
    "test_df = cap_df[train_size:]\n",
    "\n",
    "# split text and labels\n",
    "train_sentences = train_df.text_data.to_numpy()\n",
    "train_labels = train_df.profane_class.to_numpy()\n",
    "test_sentences = test_df.text_data.to_numpy()\n",
    "test_labels = test_df.profane_class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39061,), (16741,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape, test_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hoes really would sit face act like puttin world relationship fuckin whole'\n",
      " 'bullshiti guess poor moneytax money movers rate working peopleyou teabaggers traitors'\n",
      " 'details may help aid banning user vandalism continued ip country canada city nanaimo british columbia country code ca currency cad canada dollars private ip known proxy'\n",
      " 'disagreement regarding whether norman finkelstein juan cole andor memri lie andor mislead since lot noncontroversial news sources available hamas go first controversial sources appropriate describing two sides taken various controversies important matters sources heard another substantive acrosstheboard accepted source opinion directly opposed finkelsteincolememri'\n",
      " 'thats hoser eh tcoqulrtowmlb']\n",
      "[[40, 46, 11, 1220, 457, 570, 5, 16066, 167, 1565, 313, 311], [29402, 425, 948, 29403, 454, 29404, 1774, 514, 20399, 3481, 11747], [787, 37, 77, 3584, 3203, 69, 133, 2013, 227, 510, 812, 481, 29405, 624, 4987, 510, 1410, 4818, 3670, 16067, 812, 3070, 1566, 227, 479, 2385], [3204, 420, 360, 7638, 20400, 6141, 4988, 1706, 16068, 1074, 1706, 8165, 84, 267, 10535, 504, 59, 872, 3972, 16, 50, 1362, 59, 482, 2532, 90, 1836, 466, 758, 3857, 331, 1578, 59, 694, 110, 6804, 29406, 1110, 68, 266, 991, 1638, 29407], [54, 16069, 2186, 29408]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0:5])\n",
    "print(train_sequences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    39061.000000\n",
       "mean        22.090371\n",
       "std         44.589042\n",
       "min          0.000000\n",
       "25%          6.000000\n",
       "50%         10.000000\n",
       "75%         20.000000\n",
       "max       1001.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sequences = []\n",
    "for one_seq in train_sentences:\n",
    "    word_list = one_seq.split() \n",
    "    #print(len(word_list))\n",
    "    len_sequences.append(len(word_list))\n",
    "#print(len_sequences)\n",
    "pd.Series(len_sequences).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39061, 50), (16741, 50))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 50\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 32)            2800864   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,825,761\n",
      "Trainable params: 2,825,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1099/1099 - 102s - loss: 0.2401 - accuracy: 0.9047 - val_loss: 0.1711 - val_accuracy: 0.9378\n",
      "Epoch 2/20\n",
      "1099/1099 - 93s - loss: 0.0920 - accuracy: 0.9712 - val_loss: 0.1865 - val_accuracy: 0.9352\n",
      "Epoch 3/20\n",
      "1099/1099 - 95s - loss: 0.0569 - accuracy: 0.9846 - val_loss: 0.2181 - val_accuracy: 0.9401\n",
      "Epoch 4/20\n",
      "1099/1099 - 52s - loss: 0.0420 - accuracy: 0.9885 - val_loss: 0.2555 - val_accuracy: 0.9409\n",
      "Epoch 5/20\n",
      "1099/1099 - 43s - loss: 0.0370 - accuracy: 0.9914 - val_loss: 0.2191 - val_accuracy: 0.9399\n",
      "Epoch 6/20\n",
      "1099/1099 - 46s - loss: 0.0283 - accuracy: 0.9933 - val_loss: 0.2131 - val_accuracy: 0.9386\n",
      "Epoch 7/20\n",
      "1099/1099 - 51s - loss: 0.0230 - accuracy: 0.9949 - val_loss: 0.2239 - val_accuracy: 0.9399\n",
      "Epoch 8/20\n",
      "1099/1099 - 49s - loss: 0.0220 - accuracy: 0.9942 - val_loss: 0.2879 - val_accuracy: 0.9340\n",
      "Epoch 9/20\n",
      "1099/1099 - 48s - loss: 0.0250 - accuracy: 0.9938 - val_loss: 0.2749 - val_accuracy: 0.9304\n",
      "Epoch 10/20\n",
      "1099/1099 - 52s - loss: 0.0219 - accuracy: 0.9935 - val_loss: 0.3157 - val_accuracy: 0.9309\n",
      "Epoch 11/20\n",
      "1099/1099 - 47s - loss: 0.0147 - accuracy: 0.9961 - val_loss: 0.3240 - val_accuracy: 0.9309\n",
      "Epoch 12/20\n",
      "1099/1099 - 48s - loss: 0.0124 - accuracy: 0.9966 - val_loss: 0.3689 - val_accuracy: 0.9314\n",
      "Epoch 13/20\n",
      "1099/1099 - 60s - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.3972 - val_accuracy: 0.9112\n",
      "Epoch 14/20\n",
      "1099/1099 - 56s - loss: 0.0156 - accuracy: 0.9957 - val_loss: 0.3787 - val_accuracy: 0.9281\n",
      "Epoch 15/20\n",
      "1099/1099 - 56s - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.3760 - val_accuracy: 0.9294\n",
      "Epoch 16/20\n",
      "1099/1099 - 41s - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.4141 - val_accuracy: 0.9271\n",
      "Epoch 17/20\n",
      "1099/1099 - 37s - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.3375 - val_accuracy: 0.9301\n",
      "Epoch 18/20\n",
      "1099/1099 - 40s - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.3591 - val_accuracy: 0.9209\n",
      "Epoch 19/20\n",
      "1099/1099 - 45s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.3602 - val_accuracy: 0.9301\n",
      "Epoch 20/20\n",
      "1099/1099 - 47s - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.3767 - val_accuracy: 0.9263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x292359767c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_padded, train_labels, epochs=20, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_padded)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thanks already voted'\n",
      " 'dont think per precedent set originality expression necessary copyright protection mere photograph outofcopyright twodimensional work may protected american copyright law talk majestic titan'\n",
      " 'birds feather stand peta stand sanity'\n",
      " 'reply cant fuckin block kiss royal thai ass'\n",
      " 'walter issacson president cnn footnoted winterberg recent biography einstein issacson wacko'\n",
      " 'left suggestions talk page' 'sorry youre karpinski'\n",
      " 'welcome hello welcome wikipedia appreciate encyclopedic contributions recent edits ones page gucci conform policies information see wikipedias policies vandalism limits acceptable additions youd like experiment wikis syntax please sandbox rather articles still questions new contributors help page write helpme message along question someone along answer shortly may also find following pages useful general introduction wikipedia five pillars wikipedia help pages tutorial hope enjoy editing wikipedian please sign name talk pages using four tildes automatically produce name date feel free write note bottom user talkdawn bardmy talk page want get touch welcome'\n",
      " 'yo hoe get slayed'\n",
      " 'july last warning keep vandaliziing work serious admins spend days nights editing good rating able block banned blocked hope afraid bye']\n",
      "[0 0 0 1 0 0 0 0 1 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[10:20])\n",
    "\n",
    "print(test_labels[10:20])\n",
    "print(predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9218087330505943"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9273858921161826"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9141104294478528"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
